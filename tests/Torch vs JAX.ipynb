{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70c7b81c-78df-4883-bd4c-1e111ddcefdc",
   "metadata": {},
   "source": [
    "# Spline base functions tests\n",
    "\n",
    "Note to self: we should be performing grid augmentation outside of the function call, unlike what they do in pykan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e079623-0b59-484a-845b-6f1fdcb0a3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim=3\n",
    "out_dim=2\n",
    "G=10\n",
    "k=3\n",
    "grid_range=[-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95ed331b-abf4-4ee2-a02f-ea05a9c36f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# pykan implementation\n",
    "def B_batch(x, grid, k=3):\n",
    "\n",
    "    grid = grid.unsqueeze(dim=2)\n",
    "    x = x.unsqueeze(dim=1)\n",
    "\n",
    "    if k == 0:\n",
    "        value = (x >= grid[:, :-1]) * (x < grid[:, 1:])\n",
    "    else:\n",
    "        B_km1 = B_batch(x[:, 0], grid=grid[:, :, 0], k=k - 1)\n",
    "        value = (x - grid[:, :-(k + 1)]) / (grid[:, k:-1] - grid[:, :-(k + 1)]) * B_km1[:, :-1] + (grid[:, k + 1:] - x) / (grid[:, k + 1:] - grid[:, 1:(-k)]) * B_km1[:, 1:]\n",
    "    \n",
    "    return value\n",
    "\n",
    "# efficientkan implementation\n",
    "def b_splines(x, grid, K=3):\n",
    "    x = x.unsqueeze(-1)\n",
    "    bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).float()\n",
    "    for k in range(1, K + 1):\n",
    "        bases = (\n",
    "            (x - grid[:, : -(k + 1)])\n",
    "            / (grid[:, k:-1] - grid[:, : -(k + 1)])\n",
    "            * bases[:, :, :-1]\n",
    "        ) + (\n",
    "            (grid[:, k + 1 :] - x)\n",
    "            / (grid[:, k + 1 :] - grid[:, 1:(-k)])\n",
    "            * bases[:, :, 1:]\n",
    "        )\n",
    "\n",
    "    return bases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cd2d894-d646-434c-bad0-6d6166d0984e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 11])\n",
      "torch.Size([6, 17])\n"
     ]
    }
   ],
   "source": [
    "# Sample points\n",
    "x = torch.normal(0,1,size=(in_dim*out_dim, 100))\n",
    "# Sample grid\n",
    "grid = torch.einsum('i,j->ij', torch.ones(in_dim*out_dim), torch.linspace(grid_range[0], grid_range[1], steps=G + 1))\n",
    "print(grid.shape)\n",
    "k = 3\n",
    "# Grid augmentation\n",
    "h = (grid[:, [-1]] - grid[:, [0]]) / (grid.shape[1] - 1)\n",
    "for i in range(k):\n",
    "    grid = torch.cat([grid[:, [0]] - h, grid], dim=1)\n",
    "    grid = torch.cat([grid, grid[:, [-1]] + h], dim=1)\n",
    "print(grid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2fe64fd-f38c-4f8c-b247-02c086bae37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes before permutation:\n",
      "torch.Size([6, 13, 100])\n",
      "torch.Size([100, 6, 13])\n",
      "Shape of second tensor after permutation:\n",
      "torch.Size([6, 13, 100])\n"
     ]
    }
   ],
   "source": [
    "method_1 = B_batch(x,grid,k)\n",
    "method_2 = b_splines(x.T,grid,k)\n",
    "\n",
    "print(\"Shapes before permutation:\")\n",
    "print(method_1.shape)\n",
    "print(method_2.shape)\n",
    "\n",
    "# Permute to get same shapes\n",
    "method_2 = method_2.permute(1, 2, 0)\n",
    "print(\"Shape of second tensor after permutation:\")\n",
    "print(method_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c21bcc5a-8bad-4c7c-b9f2-e4694a6a54ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two object have 7800 out of 7800 values equal.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The two object have {(method_1 == method_2).sum()} out of {method_2.flatten().shape[0]} values equal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295d80f9-f6be-4ed9-89ef-75b5d2300a8f",
   "metadata": {},
   "source": [
    "Timing to show why the second case is better, as it does not involve recursive function calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9b3160f-9745-44eb-a452-bdc4c6157199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pykan implementation: 7.8689660001546144\n",
      "efficientkan implementation: 7.646361299790442\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "# Make bigger arrays\n",
    "x_big = torch.normal(0,1,size=(111, 1000))\n",
    "grid_big = torch.einsum('i,j->ij', torch.ones(111), torch.linspace(grid_range[0], grid_range[1], steps=15))\n",
    "kappa = 10\n",
    "h = (grid_big[:, [-1]] - grid_big[:, [0]]) / (grid_big.shape[1] - 1)\n",
    "for i in range(kappa):\n",
    "    grid_big = torch.cat([grid_big[:, [0]] - h, grid_big], dim=1)\n",
    "    grid_big = torch.cat([grid_big, grid_big[:, [-1]] + h], dim=1)\n",
    "\n",
    "# Wrappers for timing\n",
    "def timed_function1():\n",
    "    return B_batch(x_big, grid_big, kappa)\n",
    "\n",
    "def timed_function2():\n",
    "    return b_splines(x_big.T, grid_big, kappa)\n",
    "\n",
    "elapsed_time_1 = timeit.timeit(timed_function1, number=100)\n",
    "print(f\"pykan implementation: {elapsed_time_1}\")\n",
    "elapsed_time_2 = timeit.timeit(timed_function2, number=100)\n",
    "print(f\"efficientkan implementation: {elapsed_time_2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b98d8c-1618-4305-9c1b-6e575e226f9e",
   "metadata": {},
   "source": [
    "For some reason it is not faster. In any case, let's try to write these in JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bdb6b0f-7156-4652-86ad-2384887e5c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f437cbf-0cff-421a-9cd7-2597fa18de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jB_batch(x, grid, k=3):\n",
    "    grid = jnp.expand_dims(grid, axis=2)\n",
    "    x = jnp.expand_dims(x, axis=1)\n",
    "\n",
    "    if k == 0:\n",
    "        value = (x >= grid[:, :-1]) & (x < grid[:, 1:])\n",
    "    else:\n",
    "        B_km1 = jB_batch(x[:, 0], grid=grid[:, :, 0], k=k - 1)\n",
    "        value = ((x - grid[:, :-(k + 1)]) / (grid[:, k:-1] - grid[:, :-(k + 1)])) * B_km1[:, :-1] + ((grid[:, k + 1:] - x) / (grid[:, k + 1:] - grid[:, 1:(-k)])) * B_km1[:, 1:]\n",
    "    \n",
    "    return value.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61757058-af35-4610-bbb7-b760ea43f3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jb_splines(x, grid, K=3):\n",
    "    x = jnp.expand_dims(x, axis=-1)\n",
    "    bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).astype(float)\n",
    "    \n",
    "    for k in range(1, K+1):\n",
    "        left_term = (x - grid[:, :-(k + 1)]) / (grid[:, k:-1] - grid[:, :-(k + 1)])\n",
    "        right_term = (grid[:, k + 1:] - x) / (grid[:, k + 1:] - grid[:, 1:(-k)])\n",
    "        \n",
    "        bases = left_term * bases[:, :, :-1] + right_term * bases[:, :, 1:]\n",
    "\n",
    "    return bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca7424c-4bdd-477e-86a6-10c3773375ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf4a2e76-38e9-4909-846c-7c3db45b48bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pykan - jax: 17.589296099729836\n"
     ]
    }
   ],
   "source": [
    "# Convert to jnp arrays\n",
    "jx_big = jnp.array(x_big.numpy())\n",
    "jgrid_big = jnp.array(grid_big.numpy())\n",
    "\n",
    "def jbatch_time():\n",
    "    return jB_batch(jx_big, jgrid_big, kappa)\n",
    "\n",
    "jelapsed_time_1 = timeit.timeit(jbatch_time, number=100)\n",
    "print(f\"pykan - jax: {jelapsed_time_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8111d2bb-4d76-40e7-9e8d-e8e9ca8464c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficientkan - jax: 16.92286629974842\n"
     ]
    }
   ],
   "source": [
    "def jsplines_time():\n",
    "    return jb_splines(jx_big.T, jgrid_big, kappa)\n",
    "\n",
    "jelapsed_time_2 = timeit.timeit(jsplines_time, number=100)\n",
    "print(f\"efficientkan - jax: {jelapsed_time_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26db7079-a8b2-48cc-a71e-82ff2a0b6f47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "174ed75f-52a2-463e-aba6-a5ce172db1d4",
   "metadata": {},
   "source": [
    "Well, now we're getting the expected result that the loop is faster than the recursive call, however these times are prohibitive (3 times slower than pytorch on CPU). Let's try compiling the second version for both PyTorch and JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5b24c9-e077-4974-9474-7ce012697bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12d97626-dde8-4aa9-b2c3-5341f5841231",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def jit_b_splines(x, grid, K: int = 3):\n",
    "    x = x.unsqueeze(-1)\n",
    "    bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).float()\n",
    "    for k in range(1, K + 1):\n",
    "        left_term = (x - grid[:, :-(k + 1)]) / (grid[:, k:-1] - grid[:, :-(k + 1)])\n",
    "        right_term = (grid[:, k + 1:] - x) / (grid[:, k + 1:] - grid[:, 1:(-k)])\n",
    "        bases = left_term * bases[:, :, :-1] + right_term * bases[:, :, 1:]\n",
    "\n",
    "    return bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b35498a-2c17-48bc-b416-440df1e3484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from jax import jit\n",
    "\n",
    "# Partial applies the jit decorator with static arguments, i.e. which should be kept constant for compilation\n",
    "# but would require a re-compilation if its value changes\n",
    "# We don't expect k to change throughout a single run\n",
    "@partial(jit, static_argnums=(2,))\n",
    "def jit_jb_splines(x, grid, K=3):\n",
    "    x = jnp.expand_dims(x, axis=-1)\n",
    "    bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).astype(float)\n",
    "    \n",
    "    for k in range(1, K+1):\n",
    "        left_term = (x - grid[:, :-(k + 1)]) / (grid[:, k:-1] - grid[:, :-(k + 1)])\n",
    "        right_term = (grid[:, k + 1:] - x) / (grid[:, k + 1:] - grid[:, 1:(-k)])\n",
    "        \n",
    "        bases = left_term * bases[:, :, :-1] + right_term * bases[:, :, 1:]\n",
    "\n",
    "    return bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16fb6a65-0f6f-4266-afa8-d13de82e744e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch time: 7.70453180000186\n",
      "JAX time: 0.6737136002629995\n"
     ]
    }
   ],
   "source": [
    "def torch_jit():\n",
    "    return jit_b_splines(x_big.T, grid_big, kappa)\n",
    "\n",
    "def jax_jit():\n",
    "    return jit_jb_splines(jx_big.T, jgrid_big, kappa)\n",
    "\n",
    "time_torch = timeit.timeit(torch_jit, number=100)\n",
    "time_jax = timeit.timeit(jax_jit, number=100)\n",
    "print(f\"Torch time: {time_torch}\")\n",
    "print(f\"JAX time: {time_jax}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bea3e3b-8c8c-4ff2-959c-583b269f4a16",
   "metadata": {},
   "source": [
    "Oh well, this is why JAX is cool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9257a1-a170-4947-afcf-87e1e3a41613",
   "metadata": {},
   "source": [
    "## Other random tests\n",
    "\n",
    "Let's do some similar tests for more basic functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e03eb63b-2fa5-494d-b784-f5ea17ad6bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_matmul_pytorch(A, B):\n",
    "    return torch.bmm(A, B)\n",
    "\n",
    "def batch_matmul_jax(A, B):\n",
    "    return jnp.matmul(A, B)\n",
    "\n",
    "batch_matmul_pytorch_jit = torch.jit.script(batch_matmul_pytorch)\n",
    "batch_matmul_jax_jit = jit(batch_matmul_jax)\n",
    "\n",
    "# Create large batch matrices\n",
    "batch_size = 500\n",
    "dim = 1024\n",
    "\n",
    "# PyTorch tensors\n",
    "A_torch = torch.randn(batch_size, dim, dim, dtype=torch.float32)\n",
    "B_torch = torch.randn(batch_size, dim, dim, dtype=torch.float32)\n",
    "\n",
    "# JAX arrays\n",
    "A_jax = jnp.array(A_torch.numpy())\n",
    "B_jax = jnp.array(B_torch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2d6553f-e196-4c17-9954-924d54f85545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch time without JIT: 1.77455 seconds per loop\n",
      "JAX time without JIT: 1.85137 seconds per loop\n",
      "PyTorch time with JIT: 1.90362 seconds per loop\n",
      "JAX time with JIT: 1.88739 seconds per loop\n"
     ]
    }
   ],
   "source": [
    "# Setup number of runs\n",
    "number = 10\n",
    "\n",
    "# Time non-JIT functions\n",
    "torch_time = timeit.timeit('batch_matmul_pytorch(A_torch, B_torch)', globals=globals(), number=number)\n",
    "jax_time = timeit.timeit('batch_matmul_jax(A_jax, B_jax)', globals=globals(), number=number)\n",
    "\n",
    "# Time JIT functions\n",
    "torch_jit_time = timeit.timeit('batch_matmul_pytorch_jit(A_torch, B_torch)', globals=globals(), number=number)\n",
    "jax_jit_time = timeit.timeit('batch_matmul_jax_jit(A_jax, B_jax)', globals=globals(), number=number)\n",
    "\n",
    "# Output results\n",
    "print(f\"PyTorch time without JIT: {torch_time / number:.5f} seconds per loop\")\n",
    "print(f\"JAX time without JIT: {jax_time / number:.5f} seconds per loop\")\n",
    "print(f\"PyTorch time with JIT: {torch_jit_time / number:.5f} seconds per loop\")\n",
    "print(f\"JAX time with JIT: {jax_jit_time / number:.5f} seconds per loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfdd5dc-290b-4629-b79b-0c2f6aeb0ac6",
   "metadata": {},
   "source": [
    "Welp, here the performance is comparable. Let's try with for loops, perhaps that's where the big difference is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d57da44d-42e5-4ac8-8196-7126acbe0b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch time: 0.64673 seconds per loop\n",
      "JAX time: 0.58546 seconds per loop\n"
     ]
    }
   ],
   "source": [
    "@torch.jit.script\n",
    "def iterative_matmul_pytorch(A, B, num_iters: int = 10):  # Explicitly type the parameter\n",
    "    result = torch.zeros_like(A)\n",
    "    for _ in range(num_iters):\n",
    "        result += torch.bmm(A, B)\n",
    "        A = A * 0.99  # simulate some decay or transformation\n",
    "    return result\n",
    "\n",
    "@jit\n",
    "def iterative_matmul_jax(A, B, num_iters=10):\n",
    "    result = jnp.zeros_like(A)\n",
    "    for _ in range(num_iters):\n",
    "        result += jnp.matmul(A, B)\n",
    "        A = A * 0.99  # simulate some decay or transformation\n",
    "    return result\n",
    "\n",
    "# Using a smaller size for easier handling on CPUs\n",
    "batch_size = 100\n",
    "dim = 512\n",
    "\n",
    "# PyTorch tensors\n",
    "A_torch = torch.randn(batch_size, dim, dim, dtype=torch.float32)\n",
    "B_torch = torch.randn(batch_size, dim, dim, dtype=torch.float32)\n",
    "\n",
    "# JAX arrays\n",
    "A_jax = jnp.array(A_torch.numpy())\n",
    "B_jax = jnp.array(B_torch.numpy())\n",
    "\n",
    "# Setup number of runs\n",
    "number = 10\n",
    "\n",
    "torch_time = timeit.timeit('iterative_matmul_pytorch(A_torch, B_torch)', globals=globals(), number=number)\n",
    "jax_time = timeit.timeit('iterative_matmul_jax(A_jax, B_jax)', globals=globals(), number=number)\n",
    "\n",
    "# Output results\n",
    "print(f\"PyTorch time: {torch_time / number:.5f} seconds per loop\")\n",
    "print(f\"JAX time: {jax_time / number:.5f} seconds per loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4773518-4f78-426a-b03e-98ce1c1e61fd",
   "metadata": {},
   "source": [
    "Yeah, this is probably it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c7554f-2ff8-4147-8d99-3a72f18f8674",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
